{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb75734-d396-4131-87c1-6e3841b19eaf",
   "metadata": {},
   "source": [
    "# Loan Default Classification System\n",
    "\n",
    "en README..\n",
    "\n",
    "**Context:**\n",
    "\n",
    "**User:** Banks, SOFOM, SOFIPO, any institution that gives loans.\n",
    "\n",
    "As a bank i want to know in advancement if a client will likely default their next payment so that I can act in advance and prevent it or prepare for it. \n",
    "\n",
    "**Objective:** \n",
    "- Dado un cliente que tiene un crédito activo, quisiera saber si pagará o no, con respecto a comportamientos en algunos de sus features. \n",
    "- ¿Cuales son los features que indican que el cliente fallará? \n",
    " \n",
    "\n",
    "----\n",
    "**Tech Stack:** Python, Pandas, Numpy, Scikit-learn, Matplotlib, Gradio\n",
    "\n",
    "**Description:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ec0833-fc19-463b-9c4f-d8406b500d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub==0.4.0 in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: kagglesdk<1.0,>=0.1.14 in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from kagglehub==0.4.0) (0.1.14)\n",
      "Requirement already satisfied: packaging in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from kagglehub==0.4.0) (24.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from kagglehub==0.4.0) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from kagglehub==0.4.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from kagglehub==0.4.0) (4.67.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from kagglesdk<1.0,>=0.1.14->kagglehub==0.4.0) (6.33.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from requests->kagglehub==0.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from requests->kagglehub==0.4.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from requests->kagglehub==0.4.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from requests->kagglehub==0.4.0) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\daia prado\\anaconda3\\lib\\site-packages (from tqdm->kagglehub==0.4.0) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install kagglehub==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65cfee2-1525-46ca-9482-3ef63b758f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries. \n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27f9ea-90bb-4adf-9750-a0d078191b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"yasserh/loan-default-dataset\")\n",
    "\n",
    "df = pd.read_csv(f\"{path}/Loan_Default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca533e55-1abc-471c-a2dd-748990608497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration\n",
    "df.info()\n",
    "\n",
    "cols_w_missing_v = df.columns[df.isnull().any()]\n",
    "print()\n",
    "print(f\"Columns with missing values: {len(cols_w_missing_v)}{\"\\n\"}{cols_w_missing_v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1220ff-93c3-4bc5-906f-f311544f74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting missing values per column\n",
    "def missing_data(dataframe): \n",
    "    return dataframe.isnull().sum()\n",
    "\n",
    "missing_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf0eb7-67ef-478e-8bfb-fd32316698b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check % of missing values. If a column has more than 50% of missing values, it will be eliminated. \n",
    "missing_values = pd.DataFrame(missing_data(df), columns=[\"missing_values\"])\n",
    "missing_values[\"missing_pct\"] = missing_values/len(df)\n",
    "\n",
    "cols_to_drop = missing_values[missing_values[\"missing_pct\"] > 0.5].index\n",
    "cols_to_drop\n",
    "\n",
    "# No columns above 50% with missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d107c1-086a-4713-81ce-1a8a2a314ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for records that have more than 50% in nan and eliminate them if any. \n",
    "threshold = 0.5\n",
    "\n",
    "rows_to_drop = df.isna().mean(axis=1) > threshold\n",
    "df.loc[rows_to_drop==True].count().sum() # count for any that was above 0.5. \n",
    "\n",
    "# No records to eliminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327b699-6554-49a5-85df-c0200b1cb191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates to minimize missing values.\n",
    "df.duplicated().sum()\n",
    "\n",
    "# No duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb1c1e-0097-467e-a42b-97bfb7b7ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain non numerical columns and extract unique values to get more insights for missing values treatment. \n",
    "non_num_cols = df.select_dtypes(exclude=\"number\").columns\n",
    "print(\"Non-numerical columns:\", \"\\n\", len(non_num_cols))\n",
    "\n",
    "# Save the columns that have 2 values and a nan(if applicable). This will help transform them into binary values later. \n",
    "binary_values = {}\n",
    "\n",
    "# Explore type of information in each column. For that let's print the unique values of each. \n",
    "for record in non_num_cols: \n",
    "    \n",
    "    unique_values = df[record].unique()\n",
    "    print(\"\\n\", record, \": \", unique_values)\n",
    "    \n",
    "    if (len(unique_values) == 3 and np.any(pd.isna(unique_values))) or len(unique_values) == 2:\n",
    "        binary_values[record] = unique_values\n",
    "    \n",
    "\n",
    "print(\"\\n\", f'total of columns with binary values: {len(binary_values)}', \"\\n\", binary_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fd618-7ed9-47bb-a338-ea99b847bd55",
   "metadata": {},
   "source": [
    "The columns with missing values are: \n",
    "\n",
    "|Coumns|Missing values|\n",
    "|------|-------------|\n",
    "|loan_limit                    |3344| \n",
    "|approv_in_adv                  |908|\n",
    "|loan_purpose                   |134|\n",
    "|rate_of_interest             |36439|\n",
    "|Interest_rate_spread         |36639|\n",
    "|Upfront_charges              |39642|\n",
    "|term                            |41|\n",
    "|Neg_ammortization              |121|\n",
    "|property_value               |15098|\n",
    "|income                        |9150|\n",
    "|age                            |200|\n",
    "|submission_of_application      |200|\n",
    "|LTV                          |15098|\n",
    "|dtir1                        |24121|\n",
    "\n",
    "Some highlights: \n",
    "- In banking the following are gold features, thus the nan values can not be eliminated, since they also give certain type of information. Based on research:\n",
    "  \n",
    "  |Feature|Meaning|\n",
    "  |------|-------|\n",
    "  |\"rate_of_interest\"|not given one|\n",
    "  |\"Interest_rate_spread\"|derived from the rate_of_interest. Missing values to calculate|\n",
    "  |\"Upfront_charges\"|client with different product or not evaluated|\n",
    "  |\"dtir1\"| debt-to-income not calculated|\n",
    "  \n",
    "- \"property_value\" means that the appraisal was not made.\n",
    "- LTV derivates from property_value. LTV is calculated as *loan_amount/property_value*, which means that if we don't have the property value, we can't calculate it. In banking LTV is highly interpretable for determining default. A high LTV means a higher default risk. The missing values, will be kept. \n",
    "- \"income\" means it wasn't declared\n",
    "- \"submission_of_application\" can not be determined by any other feature. The feature has 2 values, *to_inst* and *not_inst*. *not_inst* means that the solicitation was either incomplete, it was a pre-application, or the lead was not converted. Thus since the loan never started, this records will be deleted.\n",
    "- Features that could be imputable are: \"loan_limit\", \"approv_in_adv\", \"loan_purpose\", \"term\", \"Neg_ammortization\", \"age\", \"submission_of_application\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c9a36-27a7-4b98-a5b0-e5a2b719b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete records that were not a loan\n",
    "df_loans = df.copy()\n",
    "df_loans = df_loans[df_loans[\"submission_of_application\"] != \"not_inst\"]\n",
    "\n",
    "# check for missing data reduction\n",
    "print(f\"Columns with missing values: {(missing_data(df_loans) > 0).sum()}{\"\\n\"}\")\n",
    "# no reduction of columns with missing data. \n",
    "\n",
    "missing_data(df_loans)\n",
    "# Reduction of number of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f3d43-c4b7-4d20-9745-4bd1cb9039cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want to try out different models (logistic regression and XGBoost), \n",
    "# it's important to create different datasets that can be handled by them. \n",
    "# Logistic regression = doesn't tolerate missing values and outliers\n",
    "# XGBoost = tolearates missing values and outliers\n",
    "\n",
    "df_na = df_loans.copy() # DF with native NA. No changes needed \n",
    "\n",
    "# Create the missing flags for the golden features and put the median in the nan values. \n",
    "df_no_na = df_loans.copy() # DF with no NA, but with missing flags and imputed values. \n",
    "missing_flags_cols = [\"rate_of_interest\", \"Interest_rate_spread\", \n",
    "                      \"Upfront_charges\", \"dtir1\", \"property_value\", \n",
    "                      \"LTV\", \"income\"]\n",
    "\n",
    "for col in missing_flags_cols: \n",
    "    df_na[col+\"_missing\"] = df_na[col].isna().astype(int)\n",
    "    df_no_na[col+\"_missing\"] = df_no_na[col].isna().astype(int)\n",
    "    df_no_na[col] = df_no_na[col].fillna(df_no_na[col].median())\n",
    "\n",
    "# Insert the median in numerical data. \n",
    "df_no_na[\"term\"] = df_no_na[col].fillna(df_no_na[col].median())\n",
    "\n",
    "# For categorical columns, fillna with an extra category called \"Missing\"\n",
    "missing_val_cat_cols = [\"loan_limit\", \"approv_in_adv\", \"loan_purpose\", \n",
    "                        \"Neg_ammortization\", \"submission_of_application\", \"age\"]\n",
    "for col in missing_val_cat_cols:\n",
    "    df_no_na[col] = df_no_na[col].fillna(\"Missing\")\n",
    "\n",
    "# check for missing values again in the df_no_na df. \n",
    "missing_data(df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307cbb5-9501-4ebb-98e5-56b08ce2d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers\n",
    "# Extract numeric_columns\n",
    "numeric_cols = df_no_na.select_dtypes(exclude=\"object\")\n",
    "\n",
    "# Eliminate non valuable columns (such as the ones with \"_missing\")\n",
    "numeric_cols = list(numeric_cols.columns[~numeric_cols.columns.str.contains(\"missing\")])\n",
    "del numeric_cols[:2]\n",
    "\n",
    "# Remove Status\n",
    "numeric_cols.remove(\"Status\")\n",
    "numeric_cols\n",
    "\n",
    "# Calculate IQR\n",
    "def iqr_outliers(feature):\n",
    "    q1 = feature.quantile(0.25)\n",
    "    q3 = feature.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return lower, upper\n",
    "\n",
    "# detect the outliers per column \n",
    "outlier_summary = {}\n",
    "for col in numeric_cols: \n",
    "    f = df_no_na[col].dropna()\n",
    "    lower, upper = iqr_outliers(f)\n",
    "\n",
    "    n_outliers = ((f < lower) | (f > upper)).sum()\n",
    "    outlier_summary[col] = {\n",
    "        \"lower\": lower, \n",
    "        \"upper\": upper, \n",
    "        \"n_outliers\": n_outliers,\n",
    "        \"pct_outliers\": round((n_outliers/len(f)*100), 2)\n",
    "    }\n",
    "\n",
    "possible_errors = []\n",
    "rare_values = []\n",
    "long_tales = []\n",
    "subsamples = []\n",
    "\n",
    "for key in outlier_summary.keys(): \n",
    "    if (outlier_summary[key][\"pct_outliers\"] < 0.5) and (outlier_summary[key][\"pct_outliers\"] != 0):\n",
    "        possible_errors.append(key)\n",
    "    elif (outlier_summary[key][\"pct_outliers\"] > 0.5) and (outlier_summary[key][\"pct_outliers\"] < 2): \n",
    "        rare_values.append(key)\n",
    "    elif (outlier_summary[key][\"pct_outliers\"] > 2) and (outlier_summary[key][\"pct_outliers\"] < 10):\n",
    "        long_tales.append(key)\n",
    "    else:\n",
    "        subsamples.append(key)\n",
    "\n",
    "print(f\"Possible features with errors {possible_errors}\")\n",
    "print(f\"rare values {rare_values}\")\n",
    "print(f\"long tale {long_tales}\")\n",
    "print(f\"subsamples {subsamples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64c6df-ca00-46d4-9479-3c4db2e3a7db",
   "metadata": {},
   "source": [
    "The Long tales will be the ones to watch out for a possible transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1cf3e-7cec-4e80-b575-b3414cc173ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot outliers\n",
    "def plot_outliers(feature, v_df):\n",
    "    for col in feature:\n",
    "        plt.figure(figsize=(6,2))\n",
    "        sns.boxplot(x=v_df[col])\n",
    "        plt.title(col)\n",
    "        plt.show()\n",
    "\n",
    "plot_outliers(long_tales, df_no_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c201c-246d-4578-bec5-d70fd3200ec7",
   "metadata": {},
   "source": [
    "The outliers shown above are logical, although in some circunstances like LTV, income and term present some extreme values, they represent situations that can happen in real life with some exceptional clients. Thus such cases will not be eliminated, but transformed to: \n",
    "- Reduce asymmetry\n",
    "- smooth long tales\n",
    "- reduce extreme values impacts\n",
    "- improve model's stability\n",
    "\n",
    "However LTV, dtir1 and rate_of_interest, will be winsorized instead of transformed due to the fact that these metrics are ratios. If these are transformed the linear relationship between them and the risk would be broken, eliminating the insights of risk behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259b25b-e4de-4946-8981-f086ad54e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tales_to_transform = [\"loan_amount\", \"property_value\", \"income\"]\n",
    "tales_to_winsor = [\"Interest_rate_spread\", \"LTV\", \"dtir1\"]\n",
    "\n",
    "for col in tales_to_transform:\n",
    "    df_no_na[col] = np.log1p(df_no_na[col])\n",
    "\n",
    "# plot to see the new ranges of outliers\n",
    "plot_outliers(long_tales, df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a35490-c040-4fb7-8bf5-3af96f382c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain correlations between features vs target. \n",
    "# add the status column again. \n",
    "# numeric_cols.append(\"Status\") # Run this line 1 time. \n",
    "\n",
    "# check for numerical correlations\n",
    "corr = df_no_na[numeric_cols].corr(method=\"spearman\")\n",
    "\n",
    "# plot correlations\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(corr)\n",
    "plt.title(\"Numerical correlations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e145d-7451-4b67-a54d-03a022188af4",
   "metadata": {},
   "source": [
    "There are some important correlations here between variables, but none strong enough with the target. Some of the most highliting correlated variables are: \n",
    "\n",
    "|Features|Interpretation|\n",
    "|--------|--------------|\n",
    "|income and term| Very Strong correlation| \n",
    "|property_value and loan_amount| Strong correlation|\n",
    "|term and loan_amount| Moderate correlation|\n",
    "|income and loan_amount| Moderate correlation|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b312c2d-4887-4ffc-b98a-02b37a323635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain high correlations pairs\n",
    "corr = corr.abs()\n",
    "\n",
    "high_corr_pairs = (\n",
    "    corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs[0] > 0.7]\n",
    "high_corr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703a29f-e062-408a-9ab0-8d7b6a51e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether they influence in the same proportion to the target. \n",
    "unique_values = []\n",
    "for col in high_corr_pairs.columns[0:2]:\n",
    "    unique_values.extend(high_corr_pairs[col].unique())\n",
    "\n",
    "for value in unique_values: \n",
    "    print(df_no_na.groupby(pd.qcut(df_no_na[value], 4, duplicates=\"drop\"), observed=True)[\"Status\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11501e01-32a9-4c0d-807c-180c3ee97c4b",
   "metadata": {},
   "source": [
    "Term and income show the same curves, which means a clear redundancy. Thus one of both, must be eliminated. To decide which one, let's check the roc_auc score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a050c364-a0b5-4646-b0a9-db148ff3f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define x and y\n",
    "x = df_no_na[numeric_cols[:-1]]\n",
    "y = df_no_na[numeric_cols[-1]]\n",
    "\n",
    "# Calculate AUC for 1 feature\n",
    "def univariate_auc(dfo, feature, target=\"Status\"):\n",
    "    x = df_no_na[[feature]]\n",
    "    y = df_no_na[target]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return auc\n",
    "\n",
    "auc_results = []\n",
    "\n",
    "#for col in unique_values[\"term\", \"income\"]:\n",
    "#    print(col)\n",
    "\n",
    "unique_values[\"term\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf60a1-33f0-45fe-a894-c2b216978484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for categorical correlacion\n",
    "cat_cols = df_no_na.select_dtypes(exclude=\"number\")\n",
    "cat_cols.columns\n",
    "\n",
    "for col in cat_cols: \n",
    "    print(df_no_na.groupby(col)[\"Status\"].mean())\n",
    "    \n",
    "# Obtener correlaciones solo que divide entre categorias y numericas \n",
    "\n",
    "# obten correlacion num-num y cat vs target. \n",
    "# codea el one-hot encoding \n",
    "\n",
    "# entrena \n",
    "\n",
    "# evalúa\n",
    "\n",
    "# git! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53f057-04f6-4fcc-a193-831464a479f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform features with 2 categories to binary. \n",
    "# vals = {}\n",
    "# for key in binary_values: \n",
    "#     values = binary_values[key]\n",
    "    \n",
    "#     vals[key] = {}\n",
    "#     val = 0\n",
    "    \n",
    "#     for value in values:\n",
    "        \n",
    "#         if pd.isna(value):\n",
    "#             continue\n",
    "            \n",
    "#         vals[key][value] = val\n",
    "#         val += 1 \n",
    "\n",
    "# vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517b2d3-faee-49a1-b5f1-b3ac2267a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = df.copy()\n",
    "# for col, mapping in vals.items():\n",
    "#     df_clean[col] = df[col].map(mapping)\n",
    "\n",
    "# non_num_cols = df_clean.select_dtypes(exclude=\"number\").columns\n",
    "# print(\"Non-numerical columns:\", \"\\n\", len(non_num_cols))\n",
    "\n",
    "# non_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a21b05-9cdf-419e-8495-e8952a1adf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09595688-c77d-432f-9023-6c9be25dc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which variables predict the status. \n",
    "# first numerical \n",
    "num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.drop([\"Status\", \"ID\", \"year\"])\n",
    "\n",
    "num_cols\n",
    "\n",
    "differences = []\n",
    "\n",
    "for col in num_cols:\n",
    "    \n",
    "    diff = abs(df[df==1][col].notna().median() - df[df==0][col].notna().median())\n",
    "    differences.append((col, diff))\n",
    "\n",
    "differences\n",
    "\n",
    "# categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c0e6e-5b54-49d5-8aca-be977cfe0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which variables distinguish clients that pay vs the ones that don't pay. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed95bc-1e4b-43d5-b2c7-583b993aab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the target is balanced. 0 = pays, 1 = doesn't pay \n",
    "target_counts = df_clean[\"Status\"].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = [\"pays[0]\", \"Doesn't pay[1]\"]\n",
    "y = target_counts\n",
    "\n",
    "ax.bar(x, y, color=['limegreen', 'firebrick'])\n",
    "ax.set_title(\"Number of clients with Payment default\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
